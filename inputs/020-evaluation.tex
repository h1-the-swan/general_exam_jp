\section{Evaluation of community detection
methods}\label{evaluation-of-community-detection-methods}

The lack of consensus on exactly what a community is and what is meant
to be achieved by its detection has presented problems for the
evaluation of community detection methods. Still, attempts have been
made to systematically evaluate the performance and output of different
methods.

Evaluating the results of any community detection method can be thought
in terms of either \emph{internal} or \emph{external} validity. Measures
of \emph{internal} validity evaluate the output of a clustering
algorithm according to some quality measure that uses only the
properties of this output; these include modularity
\autocite{newman_finding_2004}, minimum description length
\autocite{rosvall_map_2010}, and conductance
\autocite{leskovec_empirical_2010}. These measures are designed to
measure how well the communities identified by a method adhere to some
mathematical definition of what a proper community structure should look
like. The problem with using these measures to evaluate and compare
methods is that these measures often serve as the objective functions
for the very algorithms we want to evaluate. While it may be useful to
use these quality measures to compare algorithms that are trying to
optimize the same function, it may not be fair to compare more broadly
than this. As there is no strict mathematical definition for a
community, different algorithms use different quality functions to
surface community structure, and those algorithms that optimize for
whatever measure we are using to evaluate would have an unfair
advantage. Some of these quality measures are discussed in the previous
section on existing community detection algorithms.

Because of this, much of the work around evaluating community detection
methods has focused on \emph{external} validity measures, in which the
input is a network with a known community structure. The evaluation in
this case measures how well the community detection method matches this
ground truth. These ``gold standard'' networks used for evaluation are
either (1) synthetic benchmark networks created with planted
communities, or (2) real-world networks with known metadata which are
treated as ground-truth communities. In either case, the results of a
community detection algorithm can be evaluated against the expected
structure using some comparison measure.

\subsection{Comparison measures}\label{comparison-measures}

Evaluating a community detection method against either a synthetic
benchmark network or a real-world network with known community structure
requires some measure of comparison between the clustering found by the
method and the ground truth clustering. The popular measures that have
been adopted fall into one of three categories: (1) measures based on
\emph{pair counting}, (2) measures based on \emph{set matching}, or (3)
measures based on \emph{information theory}
\autocites{meila_comparing_2007}{vinh_information_2010}. These are all
general measures comparing data (not just network) clusterings; they
work by viewing the network as data points with communities as cluster
assignments.

\emph{Pair counting measures} work by taking every possible pair of
nodes in the network and classifying them based on their co-occurrence
in the clusterings. Each of these categories is then counted:

\begin{itemize}
\tightlist
\item
  \(N_{11}\): the number of pairs that co-occur in the same cluster in
  both clusterings
\item
  \(N_{00}\): the number of pairs that do not co-occur in either
  clustering
\item
  \(N_{10}\) or \(N_{01}\): the number of pairs that co-occur in one
  clustering but not the other.
\end{itemize}

Examples of measures that use these counts include the Fowlkes-Mallows
index \autocite{fowlkes_method_1983} and the Rand index
\autocite{rand_objective_1971}. The Rand index, for example, is the
ratio of pairs correctly classified in both clusterings to the total
number of pairs:

\[\frac{N_{11} + N_{00}}{N_{11} + N_{00} + N_{10} + N_{01}}\]

This measure has a value between zero and one, with one representing
perfect agreement between the clusterings and zero representing no
agreement whatsoever. In practice, it is rare to see values on the lower
end of this range, so a transformation is usually applied that sets a
baseline that accounts for chance---this is known as the adjusted Rand
index.

\emph{Set matching measures} compare clusterings by finding matches
between the clusters---for example, by treating the cluster assignments
as labels and calculating the classification error rate. This approach
has problems when the two clusterings to be compared have different
numbers of clusters, however. Even within the clusters that match, these
measures only consider the matched part of each cluster pair, leaving
out the parts that do not match. For these reasons, these measures are
not very widely used
\autocites{meila_comparing_2007}{vinh_information_2010}.

\emph{Information theoretic measures} use elements of information theory
to compare clusterings. The \emph{entropy} \(H(\mathcal{C})\) of
clustering \(\mathcal{C}\) is the average amount of information (in
bits) needed to encode and transmit each label. The \emph{mutual
information} between two clusterings \(\mathcal{C}\) and
\(\mathcal{C'}\) is the entropy of \(\mathcal{C}\) minus the conditional
entropy of \(\mathcal{C}\) given \(\mathcal{C'}\), or vice versa:
\(H(\mathcal{C}) - H(\mathcal{C}|\mathcal{C'}) = I(\mathcal{C}, \mathcal{C'}) = H(\mathcal{C'}) - H(\mathcal{C'}|\mathcal{C})\).
If we let \(P(k), k = 1, \ldots, K\) and \(P'(k'), k' = 1, \ldots, K'\)
be the random variables associated with the clusterings \(\mathcal{C}\)
and \(\mathcal{C'}\), respectively, and \(P(k, k')\) be the joint
probability---the probability that a point belongs to \(C_k\) in
clustering \(\mathcal{C}\) and to \(C'_{k'}\) in clustering
\(\mathcal{C'}\), then:

\[I(\mathcal{C}, \mathcal{C'}) = \sum_{k=1}^{K} \sum_{k'=1}^{K'} P(k, k') \log \frac{P(k, k')}{P(k) P'(k')}\]

The mutual information tells us, on average, how much knowing the
cluster assignment of a point in \(\mathcal{C}\) reduces our uncertainty
of which cluster it belongs to in \(\mathcal{C'}\). Several variations
of the mutual information measure have been proposed, including
normalized versions that are meant to vary between 0 (the clusterings
are independent of one another) and 1 (the clusterings are identical);
and versions adjusted for chance \autocite{vinh_information_2010}.
Another measure is the \emph{variation of information}:

\[
\begin{aligned}
VI(\mathcal{C}, \mathcal{C'}) &= H(\mathcal{C}) + H(\mathcal{C'}) - 2I(\mathcal{C}, \mathcal{C'}) \\
                                                            &= H(\mathcal{C}|\mathcal{C'}) + H(\mathcal{C'}|\mathcal{C})
\end{aligned}
\]

Finally, Lancichinetti et al. proposed a version of the normalized
mutual information that can handle the case of \emph{covers},
clusterings in which a node can be assigned to more than one cluster
\autocite{lancichinetti_detecting_2009}. While the authors point out
that their measure is not a true extension of normalized mutual
information because it gives different values when used to compare
normal ``hard'' clusterings, they contend that the difference is small
\autocite{lancichinetti_community_2009}.

\subsection{Synthetic benchmark
networks}\label{synthetic-benchmark-networks}

Evaluating community detection methods can mean comparing their results
with the expected results on some network with known community
structure, using the above similarity measures. One way to get such a
network is by using computer-generated synthetic benchmarks networks,
which are created with some notion of built-in community structure.
While there is no clear consensus on exactly what this structure should
look like, some standard benchmarks have emerged.

The \emph{planted \(\ell\)-partition model} is one method of generating
such a benchmark network. In this model, a graph is created with a
certain number (\(\ell\)) of clusters, each having the same number of
nodes. Nodes are connected with edges to other nodes in the same cluster
with probability \(p_{in}\), and to nodes in other clusters with
probability \(p_{out}\); as long as \(p_{in} > p_{out}\), the graph has
some community structure. The version of this model that has become
standard is known as the Girvan and Newman (GM) benchmark
\autocite{girvan_community_2002}. In this version, the number of
clusters is set at 4, with 32 nodes per cluster, and an average total
degree of 12. The parameter one uses to change how pronounced is the
community structure of the generated network is \(z{out}\)---the
expected external degree of a node. (Because the expected total degree
of a node is fixed, \(z_{in}\)---the expected internal degree of a
node---depends on \(z_{out}\)). As \(z_{out}\) increases, the community
structure of the graph becomes less apparent, until
\(z_{out} = z_{in} = 8\), in which case the internal and external
degrees are equal. Many algorithms are able to properly identify
communities up to this limit \autocite{fortunato_community_2010}.
\TODO{image for random networks increasing varying z}

The weakness of the GN benchmarks and the planted \(\ell\)-partition
model in general lies in assumptions that all communities have the same
size and all nodes have the same degree on average. Real-world networks
tend to exhibit skewed, power-law-like distributions for cluster size
and node degree. Lancichinetti et al. have taken steps to address these
issues with their LFR benchmark, which is generally seen as an
improvement over the GN benchmark
\autocites{lancichinetti_benchmark_2008}{fortunato_community_2010}. When
an LFR network is generated, both the degree and cluster size are
assumed to have power law distributions, with exponents \(\gamma\) and
\(\beta\), respectively. The mixing parameter \(\mu\) specifies, for
each node, the fraction of its links that go outside its community. A
graph of \(N\) vertices is generated using a configuration model which
assigns links given the proper degree sequence; the graph is then
rewired with an iterative algorithm until it can be assigned a community
structure with the proper parameters.

\subsection{Evaluation on real-world
networks}\label{evaluation-on-real-world-networks}

A second approach to evaluate community detection methods is to use some
real-world network data and compare the results of the method against
known metadata attributes, which are treated as ground-truth community
structure. The most famous of these networks is the Zachary karate club
dataset \autocite{zachary_information_1977}, which is a network
representation of the social interactions between 34 members of a
university karate club. A conflict between the club president and the
instructor resulted in the formation of two groups of members who had
allegiance to one leader or the other. Community detection methods can
be judged according to how well they are able to infer this group
division from the network structure alone. This network has become so
widely used as a benchmark in network science that researchers who are
first to use it as an example at a conference are awarded a trophy and
inducted into the prestigious ``Zachary Karate Club Club.''\footnote{See
  \url{http://networkkarate.tumblr.com/}}

\subsection{Visualization}\label{visualization}
