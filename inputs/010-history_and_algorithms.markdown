# History of community detection

Across many disciplines of science, it is common to encounter data that can best be represented as a network, with entities linked to each other in some meaningful way, such as through association or flow. These entities are represented by *nodes* or *vertices* connected to each other with *links* or *edges*. This overall representation is known as a *network* or *graph*.[^networkgraph] The idea of community detection as a research topic comes from a basic intuition that there exist in these networks groups of nodes that are structurally more related to each other than they are to members of other groups. 

[^networkgraph]: The term *graph* generally refers to a mathematical representation of data, while *network* usually has additional connotations related to the meaning and context associated with the data [@porter_communities_2009]; however, as is the case with many terms in this area, the distinction is not always made and the two are often used interchangeably.

The field of *network science* has emerged recently to study this and related topics. This field is highly interdisciplinary, comprising physicists, applied mathematicians, computer scientists, sociologists, and others. This interdisciplinarity arises both from the variety of methods that can be applied, and the breadth of potential applications, often requiring domain-specific knowledge [@porter_communities_2009]. Within this new field, the concept of *community* has been somewhat more formalized from the idea above as a group of nodes (a *subgraph*) with a high concentration of edges connecting vertices within the group, and a low concentration of edges with nodes outside the group [@fortunato_community_2010].

The earliest analyses of communities were made by social scientists in the early- to mid-twentieth century---for example Weiss and Jacobson's analysis of the organizational structure of a government agency [@weiss_method_1955]. More developments were made by computer scientists, who began developing graph partitioning algorithms in the early 1970s to apply to problems in parallel computing and circuit layout. In 2002, a seminal paper by Girvan and Newman [@girvan_community_2002] marked the entrance of the physics community, and ushered in the modern age of community detection [@lancichinetti_community_2009]. The Girvan and Newman algorithm introduced in their paper involved successively calculating the *edge betweenness*---the number of shortest path between all nodes that run along the edge---of all edges, then removing the edge with the highest betweenness and repeating. The idea is that the edges with the highest betweenness centralities are the ones that connect communities, and the communities can be separated by this divisive algorithm. This work inspired the development of modularity as a quality measure (see section on [the clustering perspective] below). Since then, the field has seen rapid growth and the development of many new methods.

# Community detection methods

What follows is an overview of some of the many community detection methods currently in use. The overview follows the taxonomy laid out in a recent paper by Schaub et al. [@schaub_many_2017]. The authors identify four different perspectives on the problem of community detection: (i) [the *cut-based perspective*](#the-cut-based-perspective), (ii) [the *(data) clustering perspective*](#the-clustering-perspective), (iii) (the *stochastic equivalence perspective*)[#the-stochastic-equivalence-perspective], and (iv) (the *dynamical perspective*)[|#the-dynamical-perspective]. The different perspectives represent different approaches to the problem, often with different kinds of data, different methods, and different goals. They also represent to some degree the different research communities that have been working on the problem.

\TODO{Add a note about how this might not be the only way to classify the problem space, but it is one way. It does not divide the methods cleanly, but neither do other classification. This is maybe because there are so many connections between methods. Link to discussion in the evaluation section on the need for splitting the problem up.}

## The cut-based perspective

Some of the earliest work in community detection was in the area of circuit layout and design. A circuit can be represented as a graph describing the signal flows between its components. The efficient layout of of a circuit depends on partitioning the circuit into a fixed number of similarly sized groups with a small number of edges between groups---these inter-group edges are known as the *cut*. Similar problems can be found in load scheduling and parallel computing, where tasks must be divided into different portions with minimal dependencies between them. These need for these methods led to the development of the Kernighan-Lin algorithm in 1970 [@kernighan_efficient_1970], which has become a classical method that is still frequently used. It starts with an initial partition and attempts to optimize a quality function $Q$ representing the difference between intra-cluster edges and inter-cluster edges, by swapping equal-sized subsets of vertices between groups. The method works best if it starts with a decent initial partition, so in modern use it is often used to refine partitions obtained using other methods [@fortunato_community_2010].

The cut-based perspective has also seen the development of spectral methods for graph partitioning. The spectrum (eigenvalues) of a graph's adjacency matrix tend to be related to the connectivity of the graph, and the associated eigenvectors can be used for both cut-based partitioning and clustering. These methods typically make use of the Laplacian matrix $L$ of a connected graph $G$: $L = D - A$ where $A$ is the adjacency matrix of $G$ and $D$ is the diagonal degree matrix with $D_{ii} = \sum_{j}{A_{ij}}$. The *Fiedler vector* is the second-smallest eigenvector associated with the Laplacian matrix $L$; the spectral bisection method uses this vector to quickly partition a graph into two groups with a low cut size [@fiedler_algebraic_1973].

## The clustering perspective

The clustering perspective comes from the world of data clustering, in which data points are thought of as having "distance" between each other based on their (dis)similarity, and the goal is to group together data point that are close to each other. For community detection, this distance is in relation to the connections between nodes in the network. This perspective is related to but different from the cut-based perspective above, which seeks to place divisions among the nodes so as to form balanced groups with weak connections between groups.

A classical method with this perspective is *hierarchical clustering*, which when used on graphs yields a hierarchical partitioning that can be viewed as a dendrogram. The common method uses an agglomerative approach in which each node starts in its own cluster, and they are joined together one by one based on some similarity measure calculated using the graph's adjacency matrix. This approach to community detection has several weaknesses. It necessarily infers a hierarchical community structure even if one does not exist; the hierarchy is not always easy to interpret; it often misclassifies nodes, especially nodes with only one neighbor, which it tends to put it in its own cluster; and it does not scale well to large networks [@fortunato_community_2010].

\TODO{conductance: originally developed with the cut-based perspective, but adapted as a local measure to be a clustering quality measure}

\TODO{modularity}

## The stochastic equivalence perspective

## The dynamical perspective

## Future/open problems

\TODO{hierarchical. overlapping.}
