\hypertarget{confidence-in-communities}{\section{Confidence in
communities}\label{confidence-in-communities}}

\protect\hyperlink{confidence-in-communities}{}

Although it is counterintuitive, completely random graphs, when viewed a
certain way, can be seen to have community structure. In a random graph,
known as an Erdős--Rényi graph \autocite{erdos_evolution_1960}, every
node has an equal probability of being linked to every other node.
Fig.~\ref{fig:randomcommunities} shows how a random graph can show group
structure. The top left shows the adjacency matrix of a random graph
with nodes in arbitrary order; this looks like random white noise. By
rearranging the order of the nodes in the same random graph, as in the
other panes of the figure, a community structure becomes apparent. This
is not actual community structure we are interested in, but rather
artifacts of the randomness. Nevertheless, community detection methods
can pick up on them and find artificial structure in what should be
random. These effects are even more pronounced for sparse graphs, a
category which includes most real-world networks we would like to
analyze \autocite{fortunato_community_2016}.

\begin{figure}
\centering
\includegraphics{img/fortunato2016_fig29_randomcommunities.jpg}
\caption{Artificial community structure in a random graph. Each of the
four graphs represent the adjacency matrix of the same 5000-node random
graph, in which every pair of nodes has equal probility of sharing a
link. The top right matrix shows the nodes in arbitrary order, and the
impression is one of random white noise. The others are simply
rearrangements of the nodes for the same random graph. A community
structure can be seen from what is actually random fluctuations in the
network construction process. Figure from
\autocite{fortunato_community_2016}.}\label{fig:randomcommunities}
\end{figure}

This raises the question: how can we be confident that the communities
detected by an algorithm reflect actual community structure, and not an
artifact of random noise? This is an open problem in network science,
but several attempts have been made to address it.

One approach is to compare the results to an appropriate null model. The
most popular null model has been the configuration model, which can
generate any configuration of a network given a number of nodes, number
of edges, and degree sequence for every node. One can use an ensemble of
these networks and compare them against empirical results. Any measure
of the graph, for example a centrality measure of a given node, can be
compared with the same measure on a number of generated null models; a
\(p\)-value can then be calculated as the fraction of model
configurations that yield the same value for the measure as the observed
network \autocite{fortunato_community_2016}. (It may be difficult to
operationalize ``same value'' here---requiring an exact match might make
this test too insensitive, but allowing for variation would introduce
more noise.) One could apply this approach to the community structure as
a whole, or to individual communities, or to community membership of
individual nodes, measuring these phenomena against their presence in an
ensemble of generated graphs.

Another approach is to consider the robustness of a given network to
perturbations in the network structure. From a conceptual standpoint,
this approach differs from the above one in that it does not consider
the network structure as having been generated from a stochastic
process, and attempt to model that process. Rather, it takes the
structure as given and observes the effects of random noise introduced
into this structure on the communities that are detected. This approach
has the advantages that it is applicable to any method, and it is not
restricted to a particular null model
\autocites{rosvall_mapping_2010}{mirshahvalad_significant_2012}.\footnote{In
  practice, however, a particular null model is often implied in a
  perturbation strategy, as can be seen in the discussed examples.}

Several strategies for perturbing networks have been proposed. Gfeller
et al. \autocite{gfeller_finding_2005} employ a strategy on weighted
graphs in which edge weights are increased or decreased by a relative
amount \(0 < \delta < 1\). After choosing \(\delta\), multiple
realizations of the graph are clustered using any method, and the
\emph{in-cluster} probability of each pair of adjacent vertices
\(p_{ij}\) is calculated as the fraction of realizations in which they
were clustered together. Edges with \(p_{ij} < \theta\) are considered
to be \emph{external} edges. This approach has as a weakness the need to
choose values for \(\delta\) and \(\theta\). The authors also provide a
way to measure the overall stability of the partition using these
in-cluster probabilities; this measure needs to be compared against a
null model.

Karrer et al. \autocite{karrer_robustness_2008} use a strategy for
unweighted graphs of network rewiring. Each edge is considered and
either left alone or, with probability \(\alpha\), it is removed and
replaced with another edge between a pair of vertices \((i, j)\) chosen
at random with probability \(p_{ij} = k_i k_j / 2m\) (\(k_i\) and
\(k_j\) are the degrees of \(i\) and \(j\); \(m\) is the number of edges
in the graph). This probability is the same one used in the null model
of modularity. The authors generated multiple graphs for different
values of \(\alpha\), and compared their clusterings to each other using
the variation of information \(V\). They then plotted \(V\) against
\(\alpha\) to see how different levels of perturbation would affect the
stability of the network structure. They compared the function
\(V(\alpha)\) against a null model---again, the null model of
modularity.

Rosvall and Bergstrom use a parametric bootstrap resampling method to
measure significance of communities. To resample the graph, each edge is
assigned a weight from a Poisson distribution with mean equal to the
original edge weight. The authors then cluster both the original and
resampled networks (any clustering method can be used; the authors used
Infomap). Each cluster of the original graph is considered, and of these
nodes the largest subset clustered in the same group in at least 95\% of
the resampled networks is deemed a significant cluster. By applying this
technique on citation graphs at different time points, the authors track
the change in the community structure over time (see subsection
``\protect\hyperlink{networks-of-scholarship}{Networks of scholarship}''
in section ``\protect\hyperlink{applications}{Applications of community
detection}'')

Mirshahvalad et al. \autocite{mirshahvalad_significant_2012} propose a
\emph{constructive} perturbation approach suitable for sparse networks.

In summary, measuring our confidence in community detection techniques
seems to be a hard problem. \ldots{}

\TODO{As fortunato points out: What is unrealistic about the typical null model is that any node can be connected to any other with equal probability. Typically large networks have nodes that have some "horizon" of other nodes that they can reasonably be expected to be connected to. "How to define such ‘‘horizons’’ and, more in general, realistic null models is still an open problem."}

\TODO{random seed: I think the louvain paper talks about this (blondel). I have also encountered some fluctuation in Infomap depending on the random seed. We can do work to assess the extent of the fluctuation, and what is causing it. We can also run the algorithm several times and take the best score.}
